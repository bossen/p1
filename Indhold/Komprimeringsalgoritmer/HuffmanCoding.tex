Komprimeringsalgoritmen "Huffman coding", er udviklet af David A. Huffman. Huffman udviklede algoritmen mens han var Ph.D studerende på MIT. I 1952 udgav han dokumentet"A Method for the Construction of Minimum-Redundancy Codes"\cite{A_Method_for}. Her beskrev Huffman hvordan hans komprimerings algoritme fungerede. Hvad han havde udviklet, var en 'lossless' (tabsfri) komprimerings metode, hvilket betyder, at der ikke vil være noget tab af information ved at komprimere. Komprimeringsmetoden er beregnet til binære systemer, og formålet med algoritmen er at få en given datamænde til at benytte et minimalt antal bit. Dette kan opnås ved at kigge på hyppigheden af de forskellige tegn i datamængden der skal komprimeres, og give de oftest fremkommende symboler få bits, og de mere sjældne symboler flere bits. Formålet er, at få det gennemsnitlige antal bits pr. symbol ned. For så at kunne få de orginale data tilbage fra den komprimerede form, kræver det selvfølgelig, at man har en form for ordbog, der beskriver hvilke tegn, der hører sammen med hvilke bits.

Dette princip kaldes entropikodning.



\begin{figure}[htb]
\centering
\includegraphics[scale=0.18]{Billeder/Huffman_tree_2.png}
\caption{Huffman tree}
%\label{fig:awesome_image}
\end{figure}